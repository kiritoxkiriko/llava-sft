### model
model_name_or_path: model/llava-hf/llava-1.5-7b-hf
#trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.05
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

### dataset
dataset: theblackcat102/llava-instruct-mix
dataset_dir: dataset
template: llava
cutoff_len: 8192
preprocessing_num_workers: 32

### output
output_dir: output/llava-1.5-7b-hf-finetuned
logging_steps: 10
save_steps: 500
overwrite_output_dir: true
plot_loss: true
report_to: tensorboard

### train
per_device_train_batch_size: 8
gradient_accumulation_steps: 2
learning_rate: 0.00005
weight_decay: 0.01
num_train_epochs: 3
lr_scheduler_type: cosine_with_restarts
warmup_ratio: 0.05
optim: adamw_torch
max_grad_norm: 1.0
fp16: true

### vision
#vision_tower: openai/clip-vit-large-patch14-336
#mm_projector_type: mlp2x_gelu
#freeze_vision_tower: true

### other
seed: 666
flash_attn: fa2
enable_liger_kernel: true
#gradient_checkpointing: true
#activation_offloading: true
#use_unsloth: true

### eval
val_size: 0.1
per_device_eval_batch_size: 8
eval_strategy: steps
eval_steps: 500

### deepspeed
#deepspeed:
#  stage: 3
#  offload_optimizer: false
#  offload_param: false
#  overlap_comm: true
#  contiguous_gradients: true
