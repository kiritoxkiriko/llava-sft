### model
model_name_or_path: model/llava-hf/llava-1.5-7b-hf
#trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: lora
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj
#lora_target: all

### dataset
dataset: theblackcat102/llava-instruct-mix
dataset_dir: dataset
template: llava
cutoff_len: 8192
preprocessing_num_workers: 64
max_samples: 10000

### output
output_dir: output/llava-1.5-7b-hf-finetuned
logging_steps: 10
save_steps: 500
overwrite_output_dir: true
plot_loss: true
report_to: none

### train
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 0.00002
weight_decay: 0.01
num_train_epochs: 4
lr_scheduler_type: cosine
warmup_ratio: 0.1
optim: adamw_torch
max_grad_norm: 1.0
fp16: true

### other
seed: 666
flash_attn: auto
gradient_checkpointing: true
#use_unsloth: true


# vision
#vision_tower: openai/clip-vit-large-patch14-336  # 使用较小分辨率的视觉编码器
#mm_projector_type: mlp2x_gelu
#mm_hidden_size: 2048
#mm_use_im_start_end: true

### eval
val_size: 0.1
per_device_eval_batch_size: 1
eval_strategy: steps
eval_steps: 500


# Disable NCCL P2P and IB for rtx4090
# export NCCL_P2P_DISABLE=1
# export NCCL_IB_DISABLE=1
# CUDA_VISIBLE_DEVICES=0,1
