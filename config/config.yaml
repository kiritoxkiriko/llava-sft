# Model configuration
#model_name_or_path: liuhaotian/llava-v1.6-vicuna-7b
model_name_or_path: model/liuhaotian/llava-v1.6-vicuna-7b
model_type: llava
template: llava_v1

# Training configuration
do_train: true
output_dir: output/llava-v1.6-vicuna-7b-finetuned
overwrite_output_dir: true

# Dataset configuration
dataset: HuggingFaceH4/llava-instruct-mix-vsft
dataset_dir: dataset
cutoff_len: 8192

# LoRA configuration
use_lora: true
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.05
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj

# Training hyperparameters
num_train_epochs: 3
per_device_train_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 2e-5
weight_decay: 0.01
warmup_ratio: 0.03
lr_scheduler_type: cosine
optim: adamw_torch
max_grad_norm: 1.0
logging_steps: 10
save_steps: 500
eval_steps: 500

# Other configurations
seed: 666
fp16: true
flash_attn: auto
gradient_checkpointing: true